# --- Top Sentiment Contributors ---
top_sentiment <- sentiment_df %>%
group_by(book) %>%
slice_max(abs(contribution), n = 20) %>%
ungroup() %>%
mutate(word = reorder_within(word, contribution, book))
sentiment_plot <- ggplot(top_sentiment, aes(x = contribution, y = word, fill = contribution > 0)) +
geom_col(show.legend = FALSE) +
facet_wrap(~book, scales = "free_y") +
scale_y_reordered() +
scale_fill_manual(values = c("TRUE" = "steelblue", "FALSE" = "tomato")) +
labs(
title = "Top Sentiment Contributors (Frequency × Score)",
x = "Contribution", y = NULL
) +
theme_minimal(base_size = 13)
print(sentiment_plot)
ggsave("EPS_Graphs/sentiment_contributors.eps", plot = sentiment_plot, device = "eps", width = 10, height = 6)
# ================================
# --- Bigram Network ---
# ================================
book_texts <- lemmatized_data %>%
group_by(book) %>%
summarise(text = paste(word, collapse = " "))
bigrams_df <- book_texts %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
separate(bigram, into = c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word,
!word2 %in% stop_words$word,
!is.na(word1), !is.na(word2)) %>%
count(book, word1, word2, sort = TRUE)
plot_bigram_graph <- function(book_name, color) {
bigram_data <- bigrams_df %>%
filter(book == book_name) %>%
slice_max(n, n = 30) %>%
select(word1, word2, n)
graph <- graph_from_data_frame(bigram_data)
ggraph(graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), arrow = grid::arrow(type = "closed", length = unit(0.15, "inches")), end_cap = circle(0.07, "inches"), show.legend = FALSE, color = color) +
geom_node_point(color = color) +
geom_node_text(aes(label = name), vjust = 1.5, hjust = 1, size = 3) +
labs(title = paste("Bigram Network for", book_name)) +
theme_minimal(base_size = 13)
}
#plot_1818 <- plot_bigram_graph("1818 Edition", "lightsalmon")
#plot_1831 <- plot_bigram_graph("1831 Edition", "turquoise3")
plot_1818 <- plot_bigram_graph("1818 Edition", "firebrick")
plot_1831 <- plot_bigram_graph("1831 Edition", "deepskyblue3")
print(plot_1818)
print(plot_1831)
ggsave("EPS_Graphs/bigram_1818.eps", plot = plot_1818, device = "eps", width = 8, height = 6)
ggsave("EPS_Graphs/bigram_1831.eps", plot = plot_1831, device = "eps", width = 8, height = 6)
# --- Load Required Libraries ---
library(pacman)
pacman::p_load(tidyverse, tidytext, gutenbergr, stringr, udpipe, textstem)
# --- Create Output Directory for EPS ---
dir.create("EPS_Graphs", showWarnings = FALSE)
# --- Download Raw Editions -----
frankenstein_1818_raw <- gutenberg_download(41445, mirror = "http://mirror.csclub.uwaterloo.ca/gutenberg/")
frankenstein_1831_raw <- gutenberg_download(42324, mirror = "http://mirror.csclub.uwaterloo.ca/gutenberg/")
# -----------------------------------------------------------
# --- Cleaned Data 1a – Standardize Metadata and Identify Structure ---
# -----------------------------------------------------------
# Function to remove Gutenberg metadata headers/footers
dynamic_remove_metadata <- function(book_text) {
book_text <- tolower(book_text)  # Convert entire text to lowercase
start_index <- which(str_detect(book_text, "start of (this )?project gutenberg"))  ## Detect start of book content
end_index <- which(str_detect(book_text, "end of (this )?project gutenberg"))      ## Detect end of book content
if (length(start_index) > 0 & length(end_index) > 0) {
book_text <- book_text[(start_index + 1):(end_index - 1)]  # Trim metadata
}
book_text[book_text != ""]  # Remove blank lines
}
# Function to extract structure (letter/chapter) from text using regex
download_and_structure_book <- function(book_id, edition_label) {
raw_book <- gutenberg_download(book_id, mirror = "http://mirror.csclub.uwaterloo.ca/gutenberg/")
text_cleaned <- dynamic_remove_metadata(raw_book$text)
book_tbl <- tibble(text = text_cleaned)
structured <- book_tbl %>%
mutate(
linenumber = row_number(),
is_letter = str_detect(text, regex("^letter\\s+[ivxlcdm]+\\.", ignore_case = TRUE)),  # Detect letters
is_chapter = str_detect(text, regex("^chapter\\s+[ivxlcdm]+\\.", ignore_case = TRUE)),  # Detect chapters
keep_next = lag(is_letter | is_chapter, default = FALSE),  # Retain text right after header
letter = cumsum(is_letter),  # Numbering letters
chapter = cumsum(is_chapter),  # Numbering chapters
book = edition_label
) %>%
filter(!is_letter & !is_chapter | keep_next) %>%  # Remove headers but keep associated text
select(book, letter, chapter, text)
return(structured)
}
# Load and merge both editions
frankenstein_1818 <- download_and_structure_book(41445, "1818 Edition")
frankenstein_1831 <- download_and_structure_book(42324, "1831 Edition")
frankenstein_combined <- bind_rows(frankenstein_1818, frankenstein_1831)
# -----------------------------------------------------------
# Pre-cleaning Summary: Token count BEFORE processing
# -----------------------------------------------------------
raw_summary <- frankenstein_combined %>%
unnest_tokens(word, text) %>%                    # Tokenize the text column into individual words
group_by(book) %>%                               # Group by book edition (1818 or 1831)
summarise(
Total_Words_Before_Cleaning = n(),             # Count total number of tokens
Distinct_Words = n_distinct(word),             # Count distinct vocabulary size
.groups = "drop"
)
print("Summary before cleaning:")
print(raw_summary)
# Save raw chapter-level text
write_csv(frankenstein_combined, "frankenstein_cleaned_chapter_text.csv")
chapter_word_counts_raw <- frankenstein_combined %>%
group_by(book, chapter) %>%
summarise(word_count = sum(str_count(text, "\\w+")), .groups = "drop")
write_csv(chapter_word_counts_raw, "chapter_word_counts_raw.csv")
# -----------------------------------------------------------
# Cleaned Data 1b – Remove Non-Textual and Redundant Elements
# -----------------------------------------------------------
frankenstein_combined <- frankenstein_combined %>%
group_by(book, letter, chapter) %>%
summarise(text = paste(text, collapse = " "), .groups = "drop") %>%
mutate(
text = text %>%
str_remove_all("\\[[:print:]+?\\]") %>%  # Remove editorial notes e.g. [Editor’s note]
str_to_lower() %>%  # Lowercase again
str_replace_all("[^a-z ]", " ") %>%  # Remove punctuation, symbols, and digits
str_squish()  # Standardize white space
)
# -----------------------------------------------------------
# Cleaned Data 1c – Text Annotation and Lemmatization via UDPipe
# -----------------------------------------------------------
# Save cleaned chapter text before annotation
write_csv(frankenstein_combined, "frankenstein_cleaned_chapter_text.csv")
# Download and load the UDPipe English model
ud_model <- udpipe_download_model(language = "english")
ud_model <- udpipe_load_model(ud_model$file_model)
# Assign a text ID for reference
frankenstein_text <- frankenstein_combined %>%
mutate(text_id = row_number()) %>%
select(text_id, book, letter, chapter, text)
# Annotate the text using UDPipe (POS, Lemmas, etc.)
ud_result <- udpipe_annotate(ud_model, x = frankenstein_text$text, doc_id = frankenstein_text$text_id)
ud_df <- as_tibble(ud_result)
# Filter and format annotated tokens
lemmatized_data <- ud_df %>%
mutate(doc_id = as.integer(doc_id)) %>%
filter(upos %in% c("NOUN", "VERB", "ADJ", "ADV")) %>%  # Focus on content words
filter(!is.na(lemma), nchar(lemma) > 2) %>%  # Remove short/missing lemmas
mutate(word = tolower(lemma)) %>%
left_join(frankenstein_text, by = c("doc_id" = "text_id")) %>%
select(book, letter, chapter, word) %>%
mutate(word = lemmatize_words(word))  # Normalize again using textstem
# -----------------------------------------------------------
# Cleaned Data 1d – Remove Stopwords and Noise
# -----------------------------------------------------------
data(stop_words)
custom_stopwords <- tibble(word = c("much", "can", "may", "shall", "upon", "however", "yet", "also", "must", "thus", "perhaps", "indeed", "chapter", "letter"))
lemmatized_data <- lemmatized_data %>%
anti_join(stop_words, by = "word") %>%  # Remove standard stopwords
anti_join(custom_stopwords, by = "word")  # Remove domain-specific noise
# -----------------------------------------------------------
# Cleaned Data 1e – Export and Structural Checks
# -----------------------------------------------------------
write_csv(lemmatized_data, "frankenstein_cleaned_lemmatized.csv")
chapter_word_counts_cleaned <- lemmatized_data %>%
group_by(book, chapter) %>%
summarise(word_count = n(), .groups = "drop")
write_csv(chapter_word_counts_cleaned, "chapter_word_counts_cleaned.csv")
# -----------------------------------------------------------
# Cleaned Data 1f – Summary Statistics After Cleaning
# -----------------------------------------------------------
summary_after <- lemmatized_data %>%
group_by(book) %>%
summarise(
`Total Tokens` = n(),
`Unique Words` = n_distinct(word),
.groups = "drop"
)
print("Summary after cleaning:")
print(summary_after)
# -----------------------------------------------------------
# Cleaned Data 1g – Chapter-wise Word Count Summary for Comparison
# -----------------------------------------------------------
# Load chapter-wise summaries
raw <- read_csv("chapter_word_counts_raw.csv") %>% mutate(stage = "Raw")
cleaned <- read_csv("chapter_word_counts_cleaned.csv") %>% mutate(stage = "Cleaned")
# Plot 1: Raw Word Counts by Chapter
plot_raw <- ggplot(raw, aes(x = chapter, y = word_count, color = book)) +
geom_line(linewidth = 1.2) +
geom_point(size = 2) +
scale_x_continuous(breaks = 1:24) +
labs(
title = "Raw Word Count per Chapter by Edition",
x = "Chapter Number", y = "Word Count", color = "Edition"
) +
theme_minimal()
# Plot 2: Cleaned Word Counts by Chapter
plot_cleaned <- ggplot(cleaned, aes(x = chapter, y = word_count, color = book)) +
geom_line(linewidth = 1.2) +
geom_point(size = 2) +
scale_x_continuous(breaks = 1:24) +
labs(
title = "Cleaned Word Count per Chapter by Edition",
x = "Chapter Number", y = "Word Count", color = "Edition"
) +
theme_minimal()
# Display plots
print(plot_raw)
print(plot_cleaned)
# Optional: Save to EPS or PNG
ggsave("EPS_Graphs/wordcount_raw_by_chapter.eps", plot = plot_raw, device = "eps", width = 8, height = 5)
ggsave("EPS_Graphs/wordcount_cleaned_by_chapter.eps", plot = plot_cleaned, device = "eps", width = 8, height = 5)
## Frankenstein Editions Comparative Analysis
# --- Load Required Libraries ---
library(pacman)
pacman::p_load(
tidyverse, tidytext, stringr, ggwordcloud, ggraph, igraph,
grid, png, udpipe, textstem, forcats
)
# --- Load Lemmatized Word-Level Data ---
lemmatized_data <- read_csv("frankenstein_cleaned_lemmatized.csv")
# ================================
# --- Zipf's Law Analysis ---
# ================================
freq_by_rank <- lemmatized_data %>%
count(book, word, sort = TRUE) %>%
group_by(book) %>%
mutate(rank = row_number(), term_frequency = n / sum(n)) %>%
ungroup()
zipf_plot <- ggplot(freq_by_rank, aes(rank, term_frequency, color = book)) +
geom_line(linewidth = 1.1) +
scale_x_log10() +
scale_y_log10() +
labs(
x = "Rank (log scale)",
y = "Term Frequency (log scale)",
title = "Zipf’s Law for Frankenstein (Lemmatized)",
color = "Edition"
) +
scale_color_manual(values = c("1818 Edition" = "lightsalmon", "1831 Edition" = "turquoise3")) +
theme_minimal(base_size = 13)
print(zipf_plot)
ggsave("EPS_Graphs/zipfs_law_frankenstein.eps", plot = zipf_plot, device = "eps", width = 8, height = 6)
# ================================
# --- Wordcloud ---
# ================================
top_words <- lemmatized_data %>%
count(book, word, sort = TRUE) %>%
group_by(book) %>%
slice_max(n, n = 100) %>%
ungroup()
set.seed(1234)
wordcloud_plot <- ggplot(top_words, aes(label = word, size = n, color = book)) +
geom_text_wordcloud_area() +
scale_size_area(max_size = 14) +
scale_color_manual(values = c("1818 Edition" = "salmon", "1831 Edition" = "turquoise3")) +
facet_wrap(~book) +
labs(title = "Wordcloud Comparison: Frankenstein Editions") +
theme_minimal(base_size = 14)
print(wordcloud_plot)
ggsave("EPS_Graphs/wordcloud_frankenstein.eps", plot = wordcloud_plot, device = "eps", width = 10, height = 7)
# ================================
# --- Top 20 Words per Edition ---
# ================================
top20_words <- lemmatized_data %>%
count(book, word, sort = TRUE) %>%
group_by(book) %>%
slice_max(n, n = 20) %>%
ungroup() %>%
mutate(word = reorder_within(word, n, book))
top20_plot <- ggplot(top20_words, aes(x = n, y = word, fill = book)) +
geom_col(show.legend = FALSE) +
facet_wrap(~book, scales = "free_y") +
scale_y_reordered() +
scale_fill_manual(values = c("1818 Edition" = "lightsalmon", "1831 Edition" = "turquoise3")) +
labs(title = "Top 20 Words per Edition", x = "Frequency", y = NULL) +
theme_minimal(base_size = 13) +
theme(strip.text = element_text(face = "bold"))
print(top20_plot)
ggsave("EPS_Graphs/top20_words_per_edition.eps", plot = top20_plot, device = "eps", width = 10, height = 6)
# ================================
# --- Interest Words by Chapter (Filtered to Chapters 1–24) ---
# ================================
# Refined 5 interest words
interest_words_refined <- c("creature", "monster", "fear", "father", "death")
# Filter and count their frequency across chapters (limit to chapters 1–24)
freq_interest_chap_refined <- lemmatized_data %>%
filter(word %in% interest_words_refined, chapter <= 24) %>%
count(book, chapter, word, sort = TRUE)
# Define color palette for 5 words (more clear and distinct)
cud_colors_refined <- c(
"creature" = "#1f77b4",  # clear blue
"monster"  = "#ff7f0e",  # bright orange
"fear"     = "#17becf",  # cyan (safe alternative to green)
"father"   = "#bcbd22",  # olive (greenish-yellow, colorblind safe)
"death"    = "#9467bd"   # purple (already good)
)
# Split by edition
freq_1818_refined <- freq_interest_chap_refined %>% filter(book == "1818 Edition")
freq_1831_refined <- freq_interest_chap_refined %>% filter(book == "1831 Edition")
# Create plotting function
plot_interest_words_refined <- function(data, edition_title) {
ggplot(data, aes(x = chapter, y = n, color = word, group = word)) +
geom_line(linewidth = 1.2) +
scale_color_manual(values = cud_colors_refined) +
scale_x_continuous(breaks = seq(1, 24, by = 1), limits = c(1, 24)) +
labs(
title = paste("Interest Word Trends -", edition_title),
x = "Chapter", y = "Frequency", color = "Word"
) +
theme_minimal(base_size = 13) +
theme(
strip.text = element_text(face = "bold"),
panel.grid.minor.x = element_blank(),
legend.position = "right"
)
}
# Plot and Save for 1818 Edition
plot_1818_refined <- plot_interest_words_refined(freq_1818_refined, "1818 Edition")
print(plot_1818_refined)
ggsave("EPS_Graphs/interest_words_by_chapter_1818_refined.eps", plot = plot_1818_refined, device = "eps", width = 10, height = 6)
# Plot and Save for 1831 Edition
plot_1831_refined <- plot_interest_words_refined(freq_1831_refined, "1831 Edition")
print(plot_1831_refined)
ggsave("EPS_Graphs/interest_words_by_chapter_1831_refined.eps", plot = plot_1831_refined, device = "eps", width = 10, height = 6)
# =====================================================
# --- Thematic Word Group Analysis---
# =====================================================
# --- Define Thematic Word Groups ---
word_theme_df <- tibble(
word = c(
# Creation & Science
"create", "creation", "science", "knowledge", "discover", "experiment",
"secret", "power", "spark", "lightning", "nature", "life", "animation",
# Emotion & Inner Conflict
"fear", "despair", "horror", "guilt", "love", "hope", "hate", "revenge", "grief",
"misery", "pain", "suffering", "passion", "regret",
# Identity & Alienation
"monster", "creature", "wretch", "abhor", "reject", "alone", "solitude",
"abandon", "outcast", "isolate", "hide", "stranger",
# Responsibility & Morality
"responsibility", "duty", "justice", "innocent", "crime", "punishment", "fate",
"curse", "conscience", "trial", "murder",
# Symbolism & Setting
"night", "ice", "darkness", "storm", "fire", "light", "cold", "mountain", "glacier",
"winter", "moon", "wind", "sky",
# Key Characters
"victor", "elizabeth", "father", "william", "justine", "walton", "henry", "frankenstein"
),
theme = c(
rep("Creation & Science", 13),
rep("Emotion & Inner Conflict", 14),
rep("Identity & Alienation", 12),
rep("Responsibility & Morality", 11),
rep("Symbolism & Setting", 13),
rep("Key Characters", 8)
)
)
# --- Join with Lemmatized Dataset to Tag Thematic Words ---
thematic_tagged <- lemmatized_data %>%
inner_join(word_theme_df, by = "word")
# Count Thematic Word Frequency per Chapter and Edition
theme_chapter_freq <- thematic_tagged %>%
group_by(theme, book, chapter) %>%
summarise(count = n(), .groups = "drop")
# Define color for editions
edition_colors <- c("1818 Edition" = "lightsalmon", "1831 Edition" = "turquoise3")
# Plot: 1 panel per theme, two lines per theme for editions
theme_plot_facet_by_theme <- ggplot(theme_chapter_freq, aes(x = chapter, y = count, color = book)) +
geom_line(linewidth = 1.1) +
facet_wrap(~theme, ncol = 2, scales = "free_y") +
scale_color_manual(values = edition_colors) +
scale_x_continuous(breaks = 1:24, limits = c(1, 24)) +
labs(
title = "Thematic Word Frequency by Chapter (Faceted by Theme)",
x = "Chapter Number", y = "Word Count", color = "Edition"
) +
theme_minimal(base_size = 13) +
theme(strip.text = element_text(face = "bold"))
# Display and save
print(theme_plot_facet_by_theme)
ggsave("EPS_Graphs/thematic_words_by_theme_facet.eps", plot = theme_plot_facet_by_theme, device = "eps", width = 12, height = 8)
# ================================
# --- TF-IDF Analysis ---
# ================================
tf_idf_df <- lemmatized_data %>%
count(book, word, sort = TRUE) %>%
bind_tf_idf(word, book, n)
top_tf_idf <- tf_idf_df %>%
group_by(book) %>%
slice_max(tf_idf, n = 10) %>%
ungroup() %>%
mutate(word = reorder_within(word, tf_idf, book))
tfidf_plot <- ggplot(top_tf_idf, aes(x = tf_idf, y = word, fill = book)) +
geom_col(show.legend = FALSE) +
facet_wrap(~book, scales = "free_y") +
scale_y_reordered() +
scale_fill_manual(values = c("1818 Edition" = "salmon", "1831 Edition" = "turquoise3")) +
labs(
title = "Top TF-IDF Words by Edition",
x = "TF-IDF Score", y = NULL
) +
theme_minimal(base_size = 13)
print(tfidf_plot)
ggsave("EPS_Graphs/tfidf_top_words.eps", plot = tfidf_plot, device = "eps", width = 10, height = 6)
# ================================
# --- Sentiment Analysis ---
# ================================
afinn <- get_sentiments("afinn") %>%
mutate(value = (((value + 5) / 10) * 2 - 1))
# --- Chapter wise Sentiment ---
word_freq <- lemmatized_data %>%
count(book, chapter, word, sort = TRUE)
### Sentiment contributions
sentiment_df <- word_freq %>%
inner_join(afinn, by = "word") %>%
mutate(contribution = n * value)
###  Chapter-wise average sentiment
sentiment_chapter <- sentiment_df %>%
group_by(book, chapter) %>%
summarise(weighted_sentiment = mean(value), .groups = "drop")
### Plot Chapter-wise Sentiment
sentiment_chapter_plot <- ggplot(sentiment_chapter, aes(x = chapter, y = weighted_sentiment, color = book)) +
geom_line(linewidth = 1) +
geom_point(size = 2) +
scale_x_continuous(breaks = 1:24) +
labs(
title = "Chapter-wise Sentiment in Frankenstein Editions",
x = "Chapter", y = "Weighted Sentiment Score", color = "Edition"
) +
theme_minimal(base_size = 13)
print(sentiment_chapter_plot)
ggsave("EPS_Graphs/sentiment_by_chapter.eps", plot = sentiment_chapter_plot, device = "eps", width = 8, height = 5)
# --- Select Top 15 Positive and Top 15 Negative Words for each edition ---
word_freq <- lemmatized_data %>%
count(book, word, sort = TRUE)
#Join with sentiment scores
sentiment_data <- word_freq %>%
inner_join(afinn, by = "word") %>%
mutate(sentiment = ifelse(value >= 0, "Positive", "Negative"))
## Select Top 15 Positive and Top 15 Negative Words for each edition
top_sentiment_words <- sentiment_data %>%
group_by(book, sentiment) %>%
slice_max(n, n = 15) %>%
ungroup() %>%
mutate(word = reorder_within(word, n, book))
## Plot
sentiment_word_plot <- ggplot(top_sentiment_words, aes(x = n, y = word, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment + book, scales = "free_y") +
scale_y_reordered() +
scale_fill_manual(values = c("Positive" = "turquoise3", "Negative" = "salmon")) +
labs(
title = "Top Sentiment Words in Frankenstein (1818 vs 1831)",
x = "Frequency", y = NULL
) +
theme_minimal(base_size = 13)
## Print and Save
print(sentiment_word_plot)
ggsave("EPS_Graphs/top_sentiment_words.eps", plot = sentiment_word_plot, device = "eps", width = 10, height = 6)
# --- Top Sentiment Contributors ---
top_sentiment <- sentiment_df %>%
group_by(book) %>%
slice_max(abs(contribution), n = 20) %>%
ungroup() %>%
mutate(word = reorder_within(word, contribution, book))
sentiment_plot <- ggplot(top_sentiment, aes(x = contribution, y = word, fill = contribution > 0)) +
geom_col(show.legend = FALSE) +
facet_wrap(~book, scales = "free_y") +
scale_y_reordered() +
scale_fill_manual(values = c("TRUE" = "steelblue", "FALSE" = "tomato")) +
labs(
title = "Top Sentiment Contributors (Frequency × Score)",
x = "Contribution", y = NULL
) +
theme_minimal(base_size = 13)
print(sentiment_plot)
ggsave("EPS_Graphs/sentiment_contributors.eps", plot = sentiment_plot, device = "eps", width = 10, height = 6)
# ================================
# --- Bigram Network ---
# ================================
book_texts <- lemmatized_data %>%
group_by(book) %>%
summarise(text = paste(word, collapse = " "))
bigrams_df <- book_texts %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
separate(bigram, into = c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word,
!word2 %in% stop_words$word,
!is.na(word1), !is.na(word2)) %>%
count(book, word1, word2, sort = TRUE)
plot_bigram_graph <- function(book_name, color) {
bigram_data <- bigrams_df %>%
filter(book == book_name) %>%
slice_max(n, n = 30) %>%
select(word1, word2, n)
graph <- graph_from_data_frame(bigram_data)
ggraph(graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), arrow = grid::arrow(type = "closed", length = unit(0.15, "inches")), end_cap = circle(0.07, "inches"), show.legend = FALSE, color = color) +
geom_node_point(color = color) +
geom_node_text(aes(label = name), vjust = 1.5, hjust = 1, size = 3) +
labs(title = paste("Bigram Network for", book_name)) +
theme_minimal(base_size = 13)
}
#plot_1818 <- plot_bigram_graph("1818 Edition", "lightsalmon")
#plot_1831 <- plot_bigram_graph("1831 Edition", "turquoise3")
plot_1818 <- plot_bigram_graph("1818 Edition", "firebrick")
plot_1831 <- plot_bigram_graph("1831 Edition", "deepskyblue3")
print(plot_1818)
print(plot_1831)
ggsave("EPS_Graphs/bigram_1818.eps", plot = plot_1818, device = "eps", width = 8, height = 6)
ggsave("EPS_Graphs/bigram_1831.eps", plot = plot_1831, device = "eps", width = 8, height = 6)
